% !TEX enableShellEscape = yes
% (The above line makes atom's latex package compile with -shell-escape
% for minted, and is just ignored by other systems.)
\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{hyperref}

% Use one or the other of these for displaying code.
% NOTE: If you get
%  ! Package minted Error: You must invoke LaTeX with the -shell-escape flag.
% and don't want to use minted, just comment out the next line
\usepackage{minted} \BeforeBeginEnvironment{minted}{\begingroup\color{black}} \AfterEndEnvironment{minted}{\endgroup} \setminted{autogobble,breaklines,breakanywhere,linenos}

\usepackage{listings}

% Colours
\definecolor{blu}{rgb}{0,0,1}
\newcommand{\blu}[1]{{\textcolor{blu}{#1}}}
\definecolor{gre}{rgb}{0,.5,0}
\newcommand{\gre}[1]{\textcolor{gre}{#1}}
\definecolor{red}{rgb}{1,0,0}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\definecolor{pointscolour}{rgb}{0.6,0.3,0}

% answer commands
\newcommand\ans[1]{\par\gre{Answer: #1}}
\newenvironment{answer}{\par\begingroup\color{gre}Answer: }{\endgroup}
\let\ask\blu
\let\update\red
\newenvironment{asking}{\begingroup\color{blu}}{\endgroup}
\newcommand\pts[1]{\textcolor{pointscolour}{[#1~points]}}

% Math
\def\R{\mathbb{R}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}

\begin{document}

\title{CPSC 340 Assignment 5 (Due 2023-11-24 at 11:59pm)}
\author{}
\date{}
\maketitle
\vspace{-6em}

\red{
\section*{Project Proposal}
Please see instructions for the project proposal at \url{https://www.students.cs.ubc.ca/~cs-340/homework/project.pdf}. This link can also be accessed from the course page at the very bottom under the heading CPSC 540 Projects.
}

\section*{Important: Submission Format \pts{5}}

Please make sure to follow the submission instructions posted on the course website.
\ask{We will deduct marks if the submission format is incorrect, or if you're not using \LaTeX{} and your submission is \emph{at all} difficult to read} -- at least these 5 points, more for egregious issues.
Compared to assignment 1, your name and student number are no longer necessary (though it's not a bad idea to include them just in case, especially if you're doing the assignment with a partner).

\vspace{1em}



\section{Kernel Logistic Regresion \pts{22}}

If you run \verb|python main.py 1| it will load a synthetic 2D data set, split it into train/validation sets, and then perform regular logistic regression and kernel logistic regression (both without an intercept term, for simplicity). You'll observe that the error values and plots generated look the same, since the kernel being used is the linear kernel (i.e., the kernel corresponding to no change of basis). Here's one of the two identical plots:
\centerfig{0.5}{figs/logReg.png}


\subsection{Implementing kernels \pts{8}}

Inside \texttt{kernels.py}, you will see classes named \texttt{PolynomialKernel} and \texttt{GaussianRBFKernel}, whose \verb|__call__| methods are yet to be written.
\begin{asking}
  Implement the polynomial kernel and the RBF kernel for logistic regression.
  Report your training/validation errors and submit the plots from \verb|utils.plot_classifier| for each case.
\end{asking}
You should use the kernel hyperparameters $p=2$ and $\sigma=0.5$ respectively,
and $\lambda=0.01$ for the regularization strength.
For the Gaussian kernel, please do \emph{not} use a $1/\sqrt{2\pi\sigma^2}$ multiplier.




\subsection{Hyperparameter search \pts{10}}

For the RBF kernel logistic regression, consider the hyperparameter values $\sigma=10^m$ for $m=-2,-1,\ldots,2$ and $\lambda=10^m$ for $m=-4,-3,\ldots,2$.
The function \verb|q1_2()| has a little bit in it already to help set up to run a grid search over the possible combination of these parameter values.
You'll need to fill in the \verb|train_errs| and \verb|val_errs| arrays
with the results on the given training and validation sets, respectively;
then the code already in the function will produce a plot of the error grids.
\ask{Submit this plot}.
Also, for each of the training and testing errors,
pick the best (or one of the best, if there's a tie) hyperparameters for that error metric,
and \ask{report the parameter values and the corresponding error, as well as a plot of the decision boundaries (plotting only the training set)}.
While you're at it, \ask{submit your code}.
To recap, for this question you should be submitting:
two decision boundary plots,
the values of two hyperparameter pairs with corresponding errors,
and your code.

Note: on the real job you might choose to use a tool like scikit-learn's \texttt{GridSearchCV} to implement the grid search, but here we are asking you to implement it yourself, by looping over the hyperparameter values.


\subsection{Reflection \pts{4}}
\ask{
Briefly discuss the best hyperparameters you found in the previous part, and their associated plots. Was the training error minimized by the values you expected, given the ways that $\sigma$ and $\lambda$ affect the fundamental tradeoff?
}

\clearpage
\section{MAP Estimation \pts{16}}

In class, we considered MAP estimation in a regression model where we assumed that:
\begin{itemize}
\item The likelihood $p(y_i \mid x_i, w)$ comes from a normal density with a mean of $w^Tx_i$ and a variance of $1$.
\item The prior for each variable $j$, $p(w_j)$, is a normal distribution with a mean of zero and a variance of $\lambda^{-1}$.
\end{itemize}
Under these assumptions, we showed that this leads to the standard L2-regularized least squares objective function,
\[
f(w) = \frac{1}{2}\norm{Xw - y}^2 + \frac \lambda 2 \norm{w}^2,
\]
which is the negative log likelihood (NLL) under these assumptions (ignoring an irrelevant constant).
\ask{For each of the alternate assumptions below, show the corresponding loss function} \pts{each 4}. Simplify your answer as much as possible, including possibly dropping additive constants.
\begin{enumerate}

\item We use a Gaussian likelihood where each datapoint has its own variance $\sigma_i^2$, and a zero-mean Laplace prior with a variance of $\lambda^{-1}$.
\[
p(y_i \mid x_i,w) = \frac{1}{\sqrt{2\sigma_i^2\pi}}\exp\left(-\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}\right), \quad p(w_j) = \frac{\lambda}{2}\exp(-\lambda|w_j|).
\]
You can use $\Sigma$ as a diagonal matrix that has the values $\sigma_i^2$ along the diagonal.


\item We use a Laplace likelihood with a mean of $w^Tx_i$ and a variance of $8$, and we use a zero-mean Gaussian prior with a variance of $\sigma^2$:
\[
p(y_i \mid x_i, w) = \frac14 \exp\left(- \frac12 |w^Tx_i - y_i| \right), \quad
p(w_j) = \frac{1}{\sqrt{2\pi} \, \sigma} \exp\left(-\frac{w_j^2}{2\sigma^2} \right).
\]



 \item We use a (very robust) student $t$ likelihood with a mean of $w^Tx_i$ and $\nu$ degrees of freedom, and a Gaussian prior with a mean of $\mu_j$ and a variance of $\lambda^{-1}$,
\[
  p(y_i \mid x_i, w) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac \nu 2\right)}
                       \left(1 + \frac{(w^T x_i - y_i)^2}{\nu} \right)^{-\frac{\nu+1}{2}}
, \quad
  p(w_j) = \sqrt{\frac{\lambda}{2\pi}} \exp\left( -\frac\lambda2 (w_j - \mu_j)^2 \right).
\]
where $\Gamma$ is the \href{https://en.wikipedia.org/wiki/Gamma_function}{gamma function} (which is always non-negative).
You can use $\mu$ as a vector whose components are $\mu_j$.


\item We use a Poisson-distributed likelihood (for the case where $y_i$ represents counts), and a uniform prior for some constant $\kappa$,
\[
p(y_i | w^Tx_i) = \frac{\exp(y_iw^Tx_i)\exp(-\exp(w^Tx_i))}{y_i!}, \quad p(w_j) \propto \kappa.
\]
(This prior is 	``improper'', since $w\in\R^d$ but $\kappa$ doesn't integrate to 1 over this domain. Nevertheless, the posterior will be a proper distribution.)

\end{enumerate}


\clearpage
\section{Principal Component Analysis \pts{19}}
\subsection{PCA by Hand \pts{6}}

Consider the following dataset, containing 5 examples with 3 features each:
\begin{center}
  \begin{tabular}{ccc}
    $x_1$ & $x_2$ & $x_3$ \\
    \hline
     4 &  1 &  3 \\
     7 &  4 &  -6 \\
     5 &  2 &  0 \\
     3 &  0 & 6 \\
     6 & 3 &  -3 \\
  \end{tabular}
\end{center}
Recall that with PCA we usually assume we centre the data before applying PCA (so it has mean zero).
We're also going to use the usual form of PCA where the PCs are normalized ($\norm{w} = 1$),
and the direction of the first PC is the one that minimizes the orthogonal distance to all data points.
\update{We're only going to consider $k = 1$ component here.}
\begin{enumerate}
  \item \ask{What is the first principal component?}
  \item \ask{What is the reconstruction loss (L2 norm squared) of the point $(2, -1, 9)$? (Show your work.)}
  \item \ask{What is the reconstruction loss (L2 norm squared) of the point $(3, 0, 4)$? (Show your work.)}
\end{enumerate}
Hint: it may help (a lot) to plot the data before you start this question.



\subsection{Data Visualization \pts{7}}

If you run \verb|python main.py 3.2|, the program will load a dataset containing 50 examples, each representing an animal.
The 85 features are traits of these animals.
The script standardizes these features and gives two unsatisfying visualizations of it.
First, it shows a plot of the matrix entries, which has too much information and thus gives little insight into the relationships between the animals.
Next it shows a scatterplot based on two random features and displays the name of 15 randomly-chosen animals.
Because of the binary features even a scatterplot matrix shows us almost nothing about the data.

In \update{\texttt{encoders.py}}, you will find \update{a class named \texttt{PCAEncoder}}, which implements the classic PCA method (orthogonal bases via SVD) for a given $k$, the number of principal components. Using this class, create a scatterplot that uses the latent features $z_i$ from the PCA model with $k=2$.
Make a scatterplot of all examples using the first column of $Z$ as the $x$-axis and the second column of $Z$ as the $y$-axis, and use \texttt{plt.annotate()} to label the points corresponding to \verb|random_is| in the scatterplot.
(It's okay if some of the text overlaps each other; a fancier visualization would try to avoid this, of course, but hopefully you can still see most of the animals.)
Do the following:
\begin{asking}
	\begin{enumerate}
		\item  Hand in your modified demo and the scatterplot.
		\item Which trait of the animals has the largest influence (absolute value) on the first principal component?
		\item Which trait of the animals has the largest influence (absolute value) on the second principal component?
    \end{enumerate}
\end{asking}


\subsection{Data Compression \pts{6}}

It is important to know how much of the information in our dataset is captured by the low-dimensional PCA representation.
In class we discussed the ``analysis'' view that PCA maximizes the variance that is explained by the PCs, and the connection between the Frobenius norm and the variance of a centred data matrix $X$.
Use this connection to answer the following:
\begin{enumerate}
	\item \ask{How much of the variance is explained by our two-dimensional representation from the previous question?}
	\item \ask{How many PCs are required to explain 50\% of the variance in the data?}
\end{enumerate}
Note: you can compute the Frobenius norm of a matrix using the function \texttt{np.linalg.norm}, among other ways. Also, note that the ``variance explained'' formula from class assumes that $X$ is already centred.


\clearpage
\section{Stochastic Gradient Descent \pts{20}}

If you run \verb|python main.py 4|, the program will do the following:
\begin{enumerate}
	\item Load the dynamics learning dataset ($n = 10000, d = 5$)
	\item Standardize the features
	\item Perform gradient descent with line search to optimize an ordinary least squares linear regression model
	\item Report the training error using \texttt{np.mean()}
	\item Produce a learning curve obtained from training
\end{enumerate}

The learning curve obtained from our \texttt{GradientDescentLineSearch} looks like this:
\centerfig{.6}{./figs/gd_line_search_curve.png}

This dataset was generated from a 2D bouncing ball simulation, where the ball is initialized with some random position and random velocity. The ball is released in a box and collides with the sides of the box, while being pulled down by the Earth's gravity. The features of $X$ are the position and the velocity of the ball at some timestep and some irrelevant noise. The label $y$ is the $y$-position of the ball at the next timestep. Your task is to train an ordinary least squares model on this data using stochastic gradient descent instead of the deterministic gradient descent.

\subsection{Batch Size of SGD \pts{5}}

In \texttt{optimizers.py}, you will find \texttt{StochasticGradient}, a \textit{wrapper} class that encapsulates another optimizer--let's call this a base optimizer.
\texttt{StochasticGradient} uses the base optimizer's \texttt{step()} method for each mini-batch to navigate the parameter space.
The constructor for \texttt{StochasticGradient} has two arguments: \texttt{batch\_size} and \texttt{learning\_rate\_getter}. The argument \texttt{learning\_rate\_getter} is an object of class \texttt{LearningRateGetter} which returns the ``current'' value learning rate based on the number of batch-wise gradient descent iterations. Currently. \texttt{ConstantLR} is the only class fully implemented.

\ask{Submit your code} from \texttt{main.py} that instantiates a linear model optimized with \texttt{StochasticGradient} taking \texttt{GradientDescent} (not line search!) as a base optimizer. Do the following:
\begin{enumerate}
		\item Use ordinary least squares objective function (no regularization).
		\item Using \texttt{ConstantLR}, set the step size to $\alpha^t = 0.0003$.
		\item Try the batch size values of $\texttt{batch\_size} \in \{1, 10, 100\}$.
\end{enumerate}
\ask{For each batch size value, use the provided training and validation sets to compute and report training and validation errors after 10 epochs of training. Compare these errors to the error obtained previously.}



\subsection{Learning Rates of SGD \pts{6}}

Implement the other unfinished \texttt{LearningRateGetter} classes, which should return the learning rate $\alpha^t$ based on the following
specifications:
\begin{enumerate}
	\item \texttt{ConstantLR}: $\alpha^t = c$.
	\item \texttt{InverseLR}: $\alpha^t = c/t$.
	\item \texttt{InverseSquaredLR}: $\alpha^t = c/t^2$.
	\item \texttt{InverseSqrtLR}: $\alpha^t = c/\sqrt{t}$.
\end{enumerate}
\ask{Submit your code for these three classes.}




\subsection{The Learning Curves (Again) \pts{9}}

Using the four learning rates, produce a plot of learning curves visualizing the behaviour of the objective function $f$ value on the $y$-axis, and the number of stochastic gradient descent epochs (at least 50) on the $x$-axis. Use a batch size of 10. Use $c = 0.1$ for every learning rate function. \blu{Submit this plot and answer the following question. Which step size functions lead to the parameters converging towards a global minimum?}




\clearpage
\section{Very-Short Answer Questions \pts{18}}
\ask{Answer each of the following questions in a sentence or two.}
\begin{enumerate}
\item Assuming we want to use the original features (no change of basis) in a linear model, what is an advantage of the ``other'' normal equations over the original normal equations?


\item In class we argued that it's possible to make a kernel version of $k$-means clustering. What would an advantage of kernels be in this context?


\item In the language of loss functions and regularization, what is the difference between MLE and MAP?


\item What is the difference between a generative model and a discriminative model?


\item In this course, we usually add an offset term to a linear model by transforming to a $Z$ with an added constant feature. How, specifically, can we add an offset term to a kernel-based linear model?


\item With PCA, is it possible for the loss to increase if $k$ is increased? Briefly justify your answer.

\item Why doesn't it make sense to do PCA with $k > d$?


\item In terms of the matrices associated with PCA ($X$, $W$, $Z$, $\hat{X}$), where would a single ``eigenface'' be stored?


\item What is an advantage and a disadvantage of using stochastic gradient over SVD when doing PCA?



\end{enumerate}
\end{document}
